{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLVdwXrGqWNf",
        "outputId": "488f6e5c-547a-4e5b-8af7-932646f5056b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cse234-w25-PA' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hao-ai-lab/cse234-w25-PA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms2YdwpOqvjl",
        "outputId": "1f36f21e-5d36-4c14-864e-573f1c01385f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cse234-w25-PA/pa2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/cse234-w25-PA/pa2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4RvPU-YtOa9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xi6x44wtgMc"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUhLUb-Ytjlp"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J40fOjQW6vjc"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 256  # Tile size in the M dimension.\n",
        "BLOCK_N = 64 # Tile size in the N dimension.\n",
        "BLOCK_K = 32 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Compute program ID and offsets for this tile\n",
        "    # -------------------------------------------------------------------------\n",
        "    pid = tl.program_id(0)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_M)\n",
        "    pid_m = pid // num_pid_m\n",
        "    pid_n = pid % num_pid_m\n",
        "    offs_m_base = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n_base = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    mask_m = offs_m_base < M\n",
        "    mask_n = offs_n_base < N\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Initialize accumulator with higher precision\n",
        "    # -------------------------------------------------------------------------\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Main loop - optimize for T4 tensor cores\n",
        "    # -------------------------------------------------------------------------\n",
        "    for k in range(0, K, BLOCK_K):\n",
        "        offs_k = k + tl.arange(0, BLOCK_K)\n",
        "        mask_k = offs_k < K\n",
        "        offs_a = (offs_m_base[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "        offs_b = (offs_k[:, None] * stride_bk + offs_n_base[None, :] * stride_bn)\n",
        "        a = tl.load(a_ptr + offs_a, mask=mask_m[:, None] & mask_k[None, :], other=0.0)\n",
        "        b = tl.load(b_ptr + offs_b, mask=mask_k[:, None] & mask_n[None, :], other=0.0)\n",
        "        acc += tl.dot(a.to(tl.float16), b.to(tl.float16))\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Load C matrix and apply fused add + ReLU\n",
        "    # -------------------------------------------------------------------------\n",
        "    offs_c = offs_m_base[:, None] * stride_cm + offs_n_base[None, :] * stride_cn\n",
        "    c = tl.load(c_ptr + offs_c, mask=mask_m[:, None] & mask_n[None, :], other=0.0)\n",
        "    acc = tl.maximum(acc + c.to(tl.float32), 0.0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Store result with optimal memory pattern\n",
        "    # -------------------------------------------------------------------------\n",
        "    offs_d = offs_m_base[:, None] * stride_dm + offs_n_base[None, :] * stride_dn\n",
        "    tl.store(d_ptr + offs_d, acc.to(tl.float16), mask=mask_m[:, None] & mask_n[None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXzfuyrcuO6k"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9LKPicGuPcJ"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzKwZa1kuQ9Q",
        "outputId": "3f946e43-dfa2-426d-d848-40c1463eae1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6250, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7285,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd6pmdH2uSrJ",
        "outputId": "79afbfd8-436d-4a34-d6ed-f13539c3a3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.76 ms\n",
            "PyTorch implementation: 0.93 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.22x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-qTCJfIuWIz",
        "outputId": "19acd78a-2bd2-401b-efc9-cfff05e630ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing configuration: M=64, N=64, K=16\n",
            "  Config (M=64, N=64, K=16): Triton=2.25ms, PyTorch=0.82ms, Speedup=0.36x\n",
            "Testing configuration: M=64, N=64, K=32\n",
            "  Config (M=64, N=64, K=32): Triton=1.34ms, PyTorch=0.90ms, Speedup=0.67x\n",
            "Testing configuration: M=64, N=64, K=64\n",
            "  Config (M=64, N=64, K=64): Triton=1.24ms, PyTorch=0.93ms, Speedup=0.75x\n",
            "Testing configuration: M=64, N=128, K=16\n",
            "  Config (M=64, N=128, K=16): Triton=0.85ms, PyTorch=0.80ms, Speedup=0.94x\n",
            "Testing configuration: M=64, N=128, K=32\n",
            "  Config (M=64, N=128, K=32): Triton=0.71ms, PyTorch=0.84ms, Speedup=1.18x\n",
            "Testing configuration: M=64, N=128, K=64\n",
            "  Config (M=64, N=128, K=64): Triton=0.76ms, PyTorch=0.83ms, Speedup=1.10x\n",
            "Testing configuration: M=64, N=256, K=16\n",
            "  Config (M=64, N=256, K=16): Triton=1.20ms, PyTorch=0.86ms, Speedup=0.72x\n",
            "Testing configuration: M=64, N=256, K=32\n",
            "  Config (M=64, N=256, K=32): Triton=1.03ms, PyTorch=0.82ms, Speedup=0.79x\n",
            "Testing configuration: M=64, N=256, K=64\n",
            "  Config (M=64, N=256, K=64): Triton=0.84ms, PyTorch=0.78ms, Speedup=0.92x\n",
            "Testing configuration: M=128, N=64, K=16\n",
            "  Config (M=128, N=64, K=16): Triton=0.94ms, PyTorch=0.78ms, Speedup=0.82x\n",
            "Testing configuration: M=128, N=64, K=32\n",
            "  Config (M=128, N=64, K=32): Triton=0.81ms, PyTorch=0.84ms, Speedup=1.04x\n",
            "Testing configuration: M=128, N=64, K=64\n",
            "  Config (M=128, N=64, K=64): Triton=0.75ms, PyTorch=0.83ms, Speedup=1.11x\n",
            "Testing configuration: M=128, N=128, K=16\n",
            "  Config (M=128, N=128, K=16): Triton=0.72ms, PyTorch=0.76ms, Speedup=1.06x\n",
            "Testing configuration: M=128, N=128, K=32\n",
            "  Config (M=128, N=128, K=32): Triton=1.15ms, PyTorch=0.80ms, Speedup=0.70x\n",
            "Testing configuration: M=128, N=128, K=64\n",
            "  Config (M=128, N=128, K=64): Triton=1.16ms, PyTorch=0.85ms, Speedup=0.73x\n",
            "Testing configuration: M=128, N=256, K=16\n",
            "  Config (M=128, N=256, K=16): Triton=2.65ms, PyTorch=0.73ms, Speedup=0.28x\n",
            "Testing configuration: M=128, N=256, K=32\n",
            "  Config (M=128, N=256, K=32): Triton=2.65ms, PyTorch=0.76ms, Speedup=0.29x\n",
            "Testing configuration: M=128, N=256, K=64\n",
            "  Config (M=128, N=256, K=64): Triton=44.28ms, PyTorch=0.73ms, Speedup=0.02x\n",
            "Testing configuration: M=256, N=64, K=16\n",
            "  Config (M=256, N=64, K=16): Triton=0.86ms, PyTorch=0.78ms, Speedup=0.90x\n",
            "Testing configuration: M=256, N=64, K=32\n",
            "  Config (M=256, N=64, K=32): Triton=0.68ms, PyTorch=0.84ms, Speedup=1.24x\n",
            "Testing configuration: M=256, N=64, K=64\n",
            "  Config (M=256, N=64, K=64): Triton=1.76ms, PyTorch=0.75ms, Speedup=0.42x\n",
            "Testing configuration: M=256, N=128, K=16\n",
            "  Config (M=256, N=128, K=16): Triton=2.34ms, PyTorch=0.75ms, Speedup=0.32x\n",
            "Testing configuration: M=256, N=128, K=32\n",
            "  Config (M=256, N=128, K=32): Triton=42.68ms, PyTorch=0.83ms, Speedup=0.02x\n",
            "Testing configuration: M=256, N=128, K=64\n",
            "  Config (M=256, N=128, K=64): Triton=50.20ms, PyTorch=0.86ms, Speedup=0.02x\n",
            "Testing configuration: M=256, N=256, K=16\n",
            "  Config (M=256, N=256, K=16): Triton=43.00ms, PyTorch=0.85ms, Speedup=0.02x\n",
            "Testing configuration: M=256, N=256, K=32\n",
            "  Config (M=256, N=256, K=32): Triton=47.09ms, PyTorch=0.93ms, Speedup=0.02x\n",
            "Testing configuration: M=256, N=256, K=64\n",
            "  Config (M=256, N=256, K=64): Triton=50.53ms, PyTorch=0.87ms, Speedup=0.02x\n",
            "\n",
            "All Configurations (by performance):\n",
            "1. BLOCK_M=256, BLOCK_N=64, BLOCK_K=32: 1.24x speedup\n",
            "2. BLOCK_M=64, BLOCK_N=128, BLOCK_K=32: 1.18x speedup\n",
            "3. BLOCK_M=128, BLOCK_N=64, BLOCK_K=64: 1.11x speedup\n",
            "4. BLOCK_M=64, BLOCK_N=128, BLOCK_K=64: 1.10x speedup\n",
            "5. BLOCK_M=128, BLOCK_N=128, BLOCK_K=16: 1.06x speedup\n",
            "6. BLOCK_M=128, BLOCK_N=64, BLOCK_K=32: 1.04x speedup\n",
            "7. BLOCK_M=64, BLOCK_N=128, BLOCK_K=16: 0.94x speedup\n",
            "8. BLOCK_M=64, BLOCK_N=256, BLOCK_K=64: 0.92x speedup\n",
            "9. BLOCK_M=256, BLOCK_N=64, BLOCK_K=16: 0.90x speedup\n",
            "10. BLOCK_M=128, BLOCK_N=64, BLOCK_K=16: 0.82x speedup\n",
            "11. BLOCK_M=64, BLOCK_N=256, BLOCK_K=32: 0.79x speedup\n",
            "12. BLOCK_M=64, BLOCK_N=64, BLOCK_K=64: 0.75x speedup\n",
            "13. BLOCK_M=128, BLOCK_N=128, BLOCK_K=64: 0.73x speedup\n",
            "14. BLOCK_M=64, BLOCK_N=256, BLOCK_K=16: 0.72x speedup\n",
            "15. BLOCK_M=128, BLOCK_N=128, BLOCK_K=32: 0.70x speedup\n",
            "16. BLOCK_M=64, BLOCK_N=64, BLOCK_K=32: 0.67x speedup\n",
            "17. BLOCK_M=256, BLOCK_N=64, BLOCK_K=64: 0.42x speedup\n",
            "18. BLOCK_M=64, BLOCK_N=64, BLOCK_K=16: 0.36x speedup\n",
            "19. BLOCK_M=256, BLOCK_N=128, BLOCK_K=16: 0.32x speedup\n",
            "20. BLOCK_M=128, BLOCK_N=256, BLOCK_K=32: 0.29x speedup\n",
            "21. BLOCK_M=128, BLOCK_N=256, BLOCK_K=16: 0.28x speedup\n",
            "22. BLOCK_M=256, BLOCK_N=256, BLOCK_K=16: 0.02x speedup\n",
            "23. BLOCK_M=256, BLOCK_N=256, BLOCK_K=32: 0.02x speedup\n",
            "24. BLOCK_M=256, BLOCK_N=128, BLOCK_K=32: 0.02x speedup\n",
            "25. BLOCK_M=256, BLOCK_N=256, BLOCK_K=64: 0.02x speedup\n",
            "26. BLOCK_M=256, BLOCK_N=128, BLOCK_K=64: 0.02x speedup\n",
            "27. BLOCK_M=128, BLOCK_N=256, BLOCK_K=64: 0.02x speedup\n",
            "\n",
            "Best configuration: BLOCK_M=256, BLOCK_N=64, BLOCK_K=32\n",
            "Best speedup: 1.24x\n",
            "\n",
            "Final verification with best configuration:\n",
            "  Config (M=256, N=64, K=32): Triton=0.76ms, PyTorch=0.95ms, Speedup=1.24x\n"
          ]
        }
      ],
      "source": [
        "# Write your grid search here.\n",
        "\n",
        "def benchmark_matmul(M, N, K, block_m, block_n, block_k, repeats=100):\n",
        "    \"\"\"Benchmark a specific configuration of block sizes.\"\"\"\n",
        "    global BLOCK_M, BLOCK_N, BLOCK_K\n",
        "    BLOCK_M, BLOCK_N, BLOCK_K = block_m, block_n, block_k\n",
        "\n",
        "    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "    c = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "    for _ in range(10):\n",
        "        _ = matmul_add_relu_fp16(a, b, c)\n",
        "        _ = reference_matmul_add_relu(a, b, c)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Benchmark Triton\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(repeats):\n",
        "        _ = matmul_add_relu_fp16(a, b, c)\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = (time.perf_counter() - start) / repeats\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Benchmark PyTorch\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(repeats):\n",
        "        _ = reference_matmul_add_relu(a, b, c)\n",
        "    torch.cuda.synchronize()\n",
        "    torch_time = (time.perf_counter() - start) / repeats\n",
        "\n",
        "    speedup = torch_time / triton_time\n",
        "    print(f\"  Config (M={block_m}, N={block_n}, K={block_k}): Triton={triton_time*1000:.2f}ms, PyTorch={torch_time*1000:.2f}ms, Speedup={speedup:.2f}x\")\n",
        "\n",
        "    return speedup\n",
        "\n",
        "def grid_search():\n",
        "    \"\"\"Run a grid search to find optimal block sizes.\"\"\"\n",
        "    block_m_options = [64, 128, 256]\n",
        "    block_n_options = [64, 128, 256]\n",
        "    block_k_options = [16, 32, 64]\n",
        "\n",
        "    best_speedup = 0\n",
        "    best_config = None\n",
        "    results = []\n",
        "\n",
        "    for block_m in block_m_options:\n",
        "        for block_n in block_n_options:\n",
        "            for block_k in block_k_options:\n",
        "                try:\n",
        "                    print(f\"Testing configuration: M={block_m}, N={block_n}, K={block_k}\")\n",
        "                    speedup = benchmark_matmul(2048, 2048, 2048, block_m, block_n, block_k)\n",
        "\n",
        "                    results.append({\n",
        "                        'block_m': block_m,\n",
        "                        'block_n': block_n,\n",
        "                        'block_k': block_k,\n",
        "                        'speedup': speedup\n",
        "                    })\n",
        "\n",
        "                    if speedup > best_speedup:\n",
        "                        best_speedup = speedup\n",
        "                        best_config = (block_m, block_n, block_k)\n",
        "\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error with configuration {(block_m, block_n, block_k)}: {e}\")\n",
        "\n",
        "    results.sort(key=lambda x: x['speedup'], reverse=True)\n",
        "\n",
        "    print(\"\\nAll Configurations (by performance):\")\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"{i+1}. BLOCK_M={result['block_m']}, BLOCK_N={result['block_n']}, BLOCK_K={result['block_k']}: {result['speedup']:.2f}x speedup\")\n",
        "\n",
        "    print(f\"\\nBest configuration: BLOCK_M={best_config[0]}, BLOCK_N={best_config[1]}, BLOCK_K={best_config[2]}\")\n",
        "    print(f\"Best speedup: {best_speedup:.2f}x\")\n",
        "\n",
        "    global BLOCK_M, BLOCK_N, BLOCK_K\n",
        "    BLOCK_M, BLOCK_N, BLOCK_K = best_config\n",
        "\n",
        "    return best_config, best_speedup\n",
        "\n",
        "best_config, best_speedup = grid_search()\n",
        "print(\"\\nBest configuration:\")\n",
        "_ = benchmark_matmul(2048, 2048, 2048, *best_config, repeats=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}